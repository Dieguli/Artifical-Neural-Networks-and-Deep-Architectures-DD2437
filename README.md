# Artifical-Neural-Networks-and-Deep-Architectures-DD2437
Project carried out during the course Artifical Neural Networks and Deep Architectures in Kungliga Tekniska HÃ¶gskolan (KTH). 

Lab 1: 

-Design and apply networks in classification,function  approximation and generalisation tasks.

-Identify key limitations of single-layer networks.

-Configure and monitor the behaviour of learning algorithms for single- and multi-layer perceptrons networks.

-Recognise  risks  associated  with  backpropagation  and  minimise  them  for robust learning of multi-layer perceptrons.

Lab 2: In this lab the focus will be on unsupervised neural network approaches that involve competitive learning and the related concept of self-organisation. In this context,  Radial-Basis Function (RBF) networks, which incorporate both unsupervised and supervised learning to address classication and regression tasks will be implemented. The second part of the lab is devoted to the most famous representative of self-organising NNs - Kohonen maps, commonly referred to self-organising maps (SOMs). These networks can be used to help visualise high-dimensional data by finding suitable low-dimensional manifolds or perform clustering in high-dimensional spaces. In both cases the objective is to organise and present complex data in an intuitive visual form understandable for humans.

Lab 3: This exercise is predominantly concerned with Hopfield networks and associative memory. The objectives are: 

-Explaining the principles underlying the operation and functionality of autoassociative networks.

-Training a Hopfield network. 

-Explain the atractor dynamics of Hoppfield networks and the concept of energy function. 

-Demonstrate how autoassociative networks can do pattern completion and noise reduction.

-Investigate the question of storage capacity  and explain features that help in creasing it in associative memories.

Lab 4: The main idea behind this exercise is to get familiar with with some of the key ingredients of deep neural network (DNN) architectures. The focus is on restricted Boltzmann machines(RBMs)and autoencoders. The main objectives are:

-Explaining key ideas underlying the learning process of RBMs and autoencoders.

-Applying basic algorithms for greedy training of RBM and autoencoder layers with the use of commonly available deep learning libraries.

-Designing multi-layer neuralnetwork architectures based on RBM and autoencoders. for classification problems.

-Studying the functionality (including generative aspects) and test the performance of low-scale deep architectures developed using RBMs(deep belief networks,DBNs)and autoencoders.
